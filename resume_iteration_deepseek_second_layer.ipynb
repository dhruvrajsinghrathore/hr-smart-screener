{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "llm = ChatOllama(model=\"deepseek-r1:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume Text:\n",
      " Dhruvraj Singh Rathore\n",
      "/ne(737)206-1179 |dhruvrajrathore2011@gmail.com |Linkedin |/gtbGithub\n",
      "Education\n",
      "Texas A&M University Aug. 2024 – Dec. 2025\n",
      "Master of Science in Data Science, CGPA: 4.0 College Station, TX\n",
      "SRM Institute of Science and Technology Jul. 2018 – May 2022\n",
      "Bachelor of Technology in Computer Science, CGPA: 3.8 Chennai, India\n",
      "Technical Skills\n",
      "Programming & Data Science : Python, SQL, Pandas, NumPy, Matplotlib, Scikit-learn, Shell Script\n",
      "Databases & Cloud Computing : MySQL, NoSQL, Redis, MongoDB, AWS Suite (EMR, S3, EC2, Lambda)\n",
      "Big Data & Machine Learning : Spark, PySpark, LLMs (Large Language Models), BERT, Llama3.2, LangChain,\n",
      "Predictive Analytics, AWS Sagemaker, RAG\n",
      "Tools & Platforms : Git/GitHub, CI/CD, Apache Airflow, Docker, Power BI, SnowFlake, Data Built Tool\n",
      "Experience\n",
      "Data Analyst Dec. 2022 – Jun. 2024\n",
      "Draup Business Solutions Bangalore, India\n",
      "•Designed and deployed high-performance ETL pipelines using PySpark and SQL on AWS EMR , improving data\n",
      "integrity by 35% and reducing processing time by 30%.\n",
      "•Developed a data quality monitoring system using Apache Airflow, automating the processing of 200M+ records\n",
      "daily , reducing data inconsistencies by 40%, and cutting manual intervention time by 50% .\n",
      "•Implemented optimized OLAP data models (star/snowflake schemas) for 20M+ job records, improving query\n",
      "performance by 40% and predictive accuracy by 25%, enabling seamless analytical integration.\n",
      "•Integrated a serverless data retrieval system using AWS Lambda, S3, and DynamoDB , streamlining ad-hoc client\n",
      "data requests and reducing response time by 30% .\n",
      "•Automated JIRA ticketing for data issues, streamlining ETL and model deployment tasks. Reduced manual effort by\n",
      "40% and sped up issue resolution by 2x, improving workflow efficiency.\n",
      "Data Scientist Aug. 2021 – Nov. 2022\n",
      "HighRadius Corporation Hyderabad, India\n",
      "•Transformed data gathering using SQL indexing and window functions for faster retrieval and aggregations, while\n",
      "leveraging lazy evaluation in P \n",
      "...\n",
      "JD Text:\n",
      " Data & Analytics Engineering Intern LiveRamp is the data collaboration platform of choice for the world’s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases—within organizations, between brands, and across its premier global network of top-quality partners. Hundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to \n",
      "\n",
      "LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements. LiveRamp is looking for a Data & Analytics Engineering Intern to join our growing data team. In this role, you will work on designing and optimizing data pipelines, enabling analytics solutions, and contributing to LiveRamp’s data infrastructure. You will collaborate with engineers, analysts, and business teams to transform raw data into actionable insights. This internship is ideal for students who are passionate about data engineering, analytics, and solving complex problems with data-driven solutions. \n",
      "\n",
      "You will:\n",
      "1. Assist in developing, maintaining, and optimizing data pipelines and data platforms. \n",
      "2. Collaborate with the engineering team to build and enhance data models and ETL processes. \n",
      "3. Support the design and implementation of analytics dashboards and reporting tools. \n",
      "4. Work with cross-functional teams to understand business requirements and translate them into technical solutions. \n",
      "5. Explore new technologies and tools to improve data processing and analytics capabilities. \n",
      "6. Contribute to data quality initiatives by monitoring and ensuring the integrity of data across systems. \n",
      "\n",
      "About you: \n",
      "1. Currently pursuing a Bachelor’s or Master’s degree in Computer Science, Data Engineering, Analytics, or a related field. Proficiency in Python and SQL for data processing and analysis. \n",
      "2. Strong understanding of data modeling, ETL concepts, and database systems (relational and non-relational). \n",
      "3. Excellent problem-solving skills, analytical mindset, and attention to detail. Ability to work collaboratively in a fast-paced, team-oriented environment. \n",
      "4. Experience with big data tools (such as Spark, Hadoop, or Kafka) is a plus but not required.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "def extract_sections(resume_text):\n",
    "    sections = {\"projects\": \"\", \"experience\": \"\"}\n",
    "    current = None\n",
    "    for line in resume_text.splitlines():\n",
    "        l = line.lower().strip()\n",
    "        if \"experience\" in l:\n",
    "            current = \"experience\"\n",
    "        elif \"project\" in l:\n",
    "            current = \"projects\"\n",
    "        elif any(x in l for x in [\"education\", \"skills\", \"summary\", \"technical\", \"certification\"]):\n",
    "            current = None\n",
    "        if current:\n",
    "            sections[current] += line.strip() + \" \"\n",
    "    return sections\n",
    "\n",
    "def compute_bert_score(resume_text, jd_text):\n",
    "    emb1 = bert_model.encode(resume_text, convert_to_tensor=True)\n",
    "    emb2 = bert_model.encode(jd_text, convert_to_tensor=True)\n",
    "    return round(util.pytorch_cos_sim(emb1, emb2).item() * 100, 2)\n",
    "\n",
    "def compute_deepseek_score_single_call(jd_text, experience_text, project_text):\n",
    "    prompt = f\"\"\"\n",
    "You are a resume evaluator.\n",
    "\n",
    "Given the job description and the candidate's resume sections below, score the candidate on a scale of 0–100 for each section separately:\n",
    "1. Experience\n",
    "2. Projects\n",
    "\n",
    "Return only the two scores in JSON format as:\n",
    "{{\"experience_score\": 88, \"project_score\": 76}}\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"{jd_text}\\\"\\\"\\\"\n",
    "\n",
    "Experience Section:\n",
    "\\\"\\\"\\\"{experience_text[:1500]}\\\"\\\"\\\"\n",
    "\n",
    "Project Section:\n",
    "\\\"\\\"\\\"{project_text[:1500]}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        reply = response.content.strip()\n",
    "        print(\"DeepSeek raw response:\", reply)\n",
    "\n",
    "        try:\n",
    "            json_start = reply.find('{')\n",
    "            json_end = reply.rfind('}') + 1\n",
    "            json_str = reply[json_start:json_end]\n",
    "            scores = json.loads(json_str)\n",
    "            exp_score = float(scores.get(\"experience_score\", 0))\n",
    "            proj_score = float(scores.get(\"project_score\", 0))\n",
    "        except Exception as e:\n",
    "            print(\"Fallback to regex due to JSON error:\", e)\n",
    "            exp_match = re.search(r'experience[^:]*[:\\-]?\\s*(\\d{1,3})', reply, re.I)\n",
    "            proj_match = re.search(r'project[^:]*[:\\-]?\\s*(\\d{1,3})', reply, re.I)\n",
    "            exp_score = float(exp_match.group(1)) if exp_match else 0\n",
    "            proj_score = float(proj_match.group(1)) if proj_match else 0\n",
    "\n",
    "        return round((exp_score + proj_score) / 2, 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"DeepSeek scoring error:\", e)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def combine_scores(bert_score, llm_score, w1=0.6, w2=0.4):\n",
    "    return round(w1 * bert_score + w2 * llm_score, 2)\n",
    "\n",
    "resume_path = \"resumes/Dhruvraj_resume_May18.pdf\"\n",
    "resume_text = extract_text_from_pdf(resume_path)\n",
    "print(\"Resume Text:\\n\", resume_text[:2000], \"\\n...\")\n",
    "\n",
    "jd_path = \"JDs/Data_engineering_intern_LiveRamp.txt\"\n",
    "with open(jd_path, 'r', encoding='utf-8') as f:\n",
    "    jd_text = f.read()\n",
    "print(\"JD Text:\\n\", jd_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek raw response: <think>\n",
      "Alright, I'm going to evaluate this candidate's experience and projects sections based on the job description provided.\n",
      "\n",
      "First, looking at the Experience section: The candidate has two roles—Data Analyst and Data Scientist. They worked with PySpark and SQL, which aligns well with LiveRamp's ETL needs. The mention of Apache Airflow is a plus since that's relevant for data quality monitoring. Implementing OLAP models sounds beneficial for their analytical skills, though I should check if the performance improvements are clearly tied to data engineering tasks.\n",
      "\n",
      "The transition from Data Analyst to Data Scientist shows growth and versatility. Using Python effectively in both roles adds value beyond SQL. The automation of JIRA tasks is a good sign for workflow efficiency, which ties into LiveRamp's need for streamlined processes. However, I'm not sure about their experience with big data tools like Spark or Kafka yet since the job emphasizes those.\n",
      "\n",
      "Now moving to the Projects section: TravelGenie project involved Python and REST APIs, along with LLMs, which is relevant given LiveRamp's focus on data analytics and integration. The use of BM25+ embeddings for ranking seems advanced but I'm not entirely sure how it directly relates to their experience. The RAG implementation in itinerary planning could show their ability to handle complex data solutions.\n",
      "\n",
      "The academic research assistant project used NLP, RAG, LangChain, and LLMs. They implemented efficient retrieval systems with FAISS and SciBERT, which is impressive. The integration of Ollama 3.2 for multi-turn summaries might be a bit specific, but it shows their capability in handling advanced language models.\n",
      "\n",
      "In the Cotton Field Detector project, using U-Net in PyTorch to achieve high IoU makes sense given their deep learning experience. Segmenting cotton fields from satellite images is a clear application of computer vision skills, which LiveRamp values for data infrastructure tasks.\n",
      "\n",
      "I need to ensure that each project aligns with the job requirements mentioned—data pipelines, ETL, analytics dashboards, etc. The candidate's ability to integrate AI and machine learning into their projects could be a strong point, especially for solving complex problems as emphasized in the job description.\n",
      "\n",
      "Overall, the candidate has relevant experience and projects, but I should look for specific mentions of big data tools like Spark or Kafka which are more prominent in the job requirements. Also, while they have some advanced AI projects, LiveRamp's focus is on data engineering and analytics, so maybe their machine learning experience could be a positive point if it relates to data-driven solutions.\n",
      "\n",
      "I think giving them an 85 for Experience because of relevant skills but needing more specific mentions of big data tools. For Projects, since they have several AI-related tasks that fit well into the data infrastructure needs, I'll give them an 80 as their work is impressive but could benefit from more direct alignment with liveRamp's ETL and data modeling specifics.\n",
      "\n",
      "So, final scores would be 85 for experience and 80 for projects.\n",
      "</think>\n",
      "\n",
      "{\"experience_score\": 85, \"project_score\": 80}\n"
     ]
    }
   ],
   "source": [
    "bert_score = compute_bert_score(resume_text, jd_text)\n",
    "\n",
    "sections = extract_sections(resume_text)\n",
    "experience_text = sections.get(\"experience\", \"\")\n",
    "project_text = sections.get(\"projects\", \"\")\n",
    "\n",
    "deepseek_score = compute_deepseek_score_single_call(jd_text, experience_text, project_text)\n",
    "\n",
    "final_score = combine_scores(bert_score, deepseek_score)## taking 36 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Scoring DataFrame:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_file</th>\n",
       "      <th>jd_file</th>\n",
       "      <th>semantic_BERT_score</th>\n",
       "      <th>DeepSeek_score</th>\n",
       "      <th>Final_Combined_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhruvraj_resume_May18.pdf</td>\n",
       "      <td>Data_engineering_intern_LiveRamp.txt</td>\n",
       "      <td>44.75</td>\n",
       "      <td>82.5</td>\n",
       "      <td>59.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 resume_file                               jd_file  \\\n",
       "0  Dhruvraj_resume_May18.pdf  Data_engineering_intern_LiveRamp.txt   \n",
       "\n",
       "   semantic_BERT_score  DeepSeek_score  Final_Combined_Score  \n",
       "0                44.75            82.5                 59.85  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([{\n",
    "    \"resume_file\": os.path.basename(resume_path),\n",
    "    \"jd_file\": os.path.basename(jd_path),\n",
    "    \"semantic_BERT_score\": bert_score,\n",
    "    \"DeepSeek_score\": deepseek_score,\n",
    "    \"Final_Combined_Score\": final_score\n",
    "}])\n",
    "\n",
    "print(\"\\nFinal Scoring DataFrame:\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Scoring DataFrame:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_file</th>\n",
       "      <th>jd_file</th>\n",
       "      <th>semantic_BERT_score</th>\n",
       "      <th>DeepSeek_score</th>\n",
       "      <th>Final_Combined_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhruvraj_resume_May18.pdf</td>\n",
       "      <td>Data_engineering_intern_LiveRamp.txt</td>\n",
       "      <td>44.75</td>\n",
       "      <td>82.5</td>\n",
       "      <td>63.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 resume_file                               jd_file  \\\n",
       "0  Dhruvraj_resume_May18.pdf  Data_engineering_intern_LiveRamp.txt   \n",
       "\n",
       "   semantic_BERT_score  DeepSeek_score  Final_Combined_Score  \n",
       "0                44.75            82.5                 63.62  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([{\n",
    "    \"resume_file\": os.path.basename(resume_path),\n",
    "    \"jd_file\": os.path.basename(jd_path),\n",
    "    \"semantic_BERT_score\": bert_score,\n",
    "    \"DeepSeek_score\": deepseek_score,\n",
    "    \"Final_Combined_Score\": final_score\n",
    "}])\n",
    "\n",
    "print(\"\\nFinal Scoring DataFrame:\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash-preview-04-17-thinking\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\nviolations {\n}\nviolations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 57\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/gemini-1.5-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m chat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstart_chat()\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a short story in exactly 50 words.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemini response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/generativeai/generative_models.py:578\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid configuration: The chat functionality does not support `candidate_count` greater than 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n\u001b[0;32m--> 578\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m    579\u001b[0m     contents\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    580\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    581\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[1;32m    582\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    583\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools_lib,\n\u001b[1;32m    584\u001b[0m     tool_config\u001b[38;5;241m=\u001b[39mtool_config,\n\u001b[1;32m    585\u001b[0m     request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    586\u001b[0m )\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m    332\u001b[0m             request,\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[1;32m    334\u001b[0m         )\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[1;32m    836\u001b[0m     request,\n\u001b[1;32m    837\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m    838\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    839\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    840\u001b[0m )\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    294\u001b[0m     target,\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[1;32m    296\u001b[0m     sleep_generator,\n\u001b[1;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[1;32m    154\u001b[0m         exc,\n\u001b[1;32m    155\u001b[0m         deadline,\n\u001b[1;32m    156\u001b[0m         sleep,\n\u001b[1;32m    157\u001b[0m         error_list,\n\u001b[1;32m    158\u001b[0m         predicate,\n\u001b[1;32m    159\u001b[0m         on_error,\n\u001b[1;32m    160\u001b[0m         exception_factory,\n\u001b[1;32m    161\u001b[0m         timeout,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\nviolations {\n}\nviolations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 57\n}\n]"
     ]
    }
   ],
   "source": [
    "## to validate the gemini model\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro\")\n",
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"Tell me a short story in exactly 50 words.\")\n",
    "print(\"Gemini response:\\n\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
