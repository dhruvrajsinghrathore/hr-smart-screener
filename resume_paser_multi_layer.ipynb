{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import re\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: PDF Text Extraction with Section Parsing\n",
    "def extract_resume_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    \n",
    "    sections = {\n",
    "        'skills': [],\n",
    "        'experience': [],\n",
    "        'education': [],\n",
    "        'projects': []\n",
    "    }\n",
    "    \n",
    "    current_section = None\n",
    "    for line in text.split('\\n'):\n",
    "        if re.match(r'(?i)^(skills|technical skills)', line):\n",
    "            current_section = 'skills'\n",
    "        elif re.match(r'(?i)^(experience|work history)', line):\n",
    "            current_section = 'experience'\n",
    "        elif re.match(r'(?i)^(education)', line):\n",
    "            current_section = 'education'\n",
    "        elif re.match(r'(?i)^(projects)', line):\n",
    "            current_section = 'projects'\n",
    "        elif current_section:\n",
    "            sections[current_section].append(line.strip())\n",
    "    \n",
    "    full_text = text.replace('\\n', ' ')\n",
    "    section_texts = {k: ' '.join(v) for k, v in sections.items()}\n",
    "    return full_text, section_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Semantic Similarity Calculation\n",
    "def calculate_semantic_similarity(text1, text2):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode([text1, text2])\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Keyword Matching Function\n",
    "def calculate_keyword_match(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Section Alignment Scoring\n",
    "def calculate_section_alignment(jd_text, resume_sections):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    jd_vector = vectorizer.fit_transform([jd_text])\n",
    "    \n",
    "    section_weights = {\n",
    "        'skills': 0.4,\n",
    "        'experience': 0.4,\n",
    "        'education': 0.15,\n",
    "        'projects': 0.05\n",
    "    }\n",
    "    \n",
    "    total_score = 0\n",
    "    for section, weight in section_weights.items():\n",
    "        if section in resume_sections and resume_sections[section]:\n",
    "            section_vector = vectorizer.transform([resume_sections[section]])\n",
    "            similarity = cosine_similarity(jd_vector, section_vector)[0][0]\n",
    "            total_score += similarity * weight\n",
    "            \n",
    "    return total_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main Execution Cell\n",
    "def main(resume_path, jd_path):\n",
    "    with open(jd_path, 'r') as f:\n",
    "        jd_text = f.read()\n",
    "    \n",
    "    resume_full, resume_sections = extract_resume_text(resume_path)\n",
    "    \n",
    "    semantic_score = calculate_semantic_similarity(resume_full, jd_text)\n",
    "    keyword_score = calculate_keyword_match(resume_full, jd_text)\n",
    "    section_score = calculate_section_alignment(jd_text, resume_sections)\n",
    "    \n",
    "    final_score = (0.7 * semantic_score) + (0.2 * keyword_score) + (0.1 * section_score)\n",
    "    \n",
    "    print(f\"Semantic Similarity: {semantic_score:.2f}\")\n",
    "    print(f\"Keyword Match: {keyword_score:.2f}\")\n",
    "    print(f\"Section Alignment: {section_score:.2f}\")\n",
    "    print(f\"\\nFinal Match Score: {final_score:.2f}/1.0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity: 0.45\n",
      "Keyword Match: 0.28\n",
      "Section Alignment: 0.61\n",
      "\n",
      "Final Match Score: 0.43/1.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Sample Execution (Uncomment to use)\n",
    "main(\"resumes/Dhruvraj_resume_May18.pdf\", \"JDs/Data_engineering_intern_LiveRamp.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF (resume)\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + '\\n'\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract text from text file (JD)\n",
    "def extract_text_from_txt(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text, is_resume=True):\n",
    "    sections = {}\n",
    "    if is_resume:\n",
    "        sections['skills'] = extract_skills(text)\n",
    "        sections['experience'] = extract_section(text, ['experience', 'work'])\n",
    "        sections['education'] = extract_section(text, ['education'])\n",
    "        sections['certifications'] = extract_section(text, ['certification', 'certificate'])\n",
    "        sections['projects'] = extract_section(text, ['project'])\n",
    "    else:\n",
    "        sections['required_skills'] = extract_section(text, ['required skills', 'requirements', 'about you'])\n",
    "        sections['responsibilities'] = extract_section(text, ['responsibilities'])\n",
    "        sections['qualifications'] = extract_section(text, ['qualifications', 'education'])\n",
    "    return sections\n",
    "\n",
    "def extract_section(text, keywords):\n",
    "    pattern = '|'.join([fr\"{k}\" for k in keywords])\n",
    "    matches = re.split(pattern, text, flags=re.IGNORECASE)\n",
    "    return matches[1] if len(matches) > 1 else text\n",
    "\n",
    "def extract_skills(text):\n",
    "    skills = re.findall(r\"(?i)\\b[A-Za-z0-9\\+\\#\\.]+\\b\", text)\n",
    "    return ', '.join([s for s in skills if len(s) > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_score(jd_sections, resume_sections, verbose=True):\n",
    "    pairs = [\n",
    "        ('required_skills', 'skills'),\n",
    "        ('responsibilities', 'experience'),\n",
    "        ('qualifications', 'education'),\n",
    "    ]\n",
    "    scores = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nüìò Semantic Similarity by Section:\")\n",
    "\n",
    "    for jd_key, res_key in pairs:\n",
    "        jd_text = jd_sections.get(jd_key, '').strip()\n",
    "        res_text = resume_sections.get(res_key, '').strip()\n",
    "\n",
    "        if jd_text and res_text:\n",
    "            jd_emb = model.encode(jd_text, convert_to_tensor=True)\n",
    "            res_emb = model.encode(res_text, convert_to_tensor=True)\n",
    "            sim = util.cos_sim(jd_emb, res_emb).item()\n",
    "            scores.append(sim)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nüîπ JD Section: {jd_key} ‚Üî Resume Section: {res_key}\")\n",
    "                print(f\"Similarity Score: {round(sim, 4)}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\nüîπ JD Section: {jd_key} ‚Üî Resume Section: {res_key}\")\n",
    "                print(\"‚ö†Ô∏è One of the sections is empty. Skipping similarity computation.\")\n",
    "\n",
    "    final_semantic = np.mean(scores) if scores else 0.0\n",
    "    if verbose:\n",
    "        print(f\"\\n‚úÖ Final Semantic Score (average): {round(final_semantic, 4)}\")\n",
    "\n",
    "    return final_semantic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_score(jd_text, resume_text, verbose=True):\n",
    "    jd_keywords = set(re.findall(r\"\\b[a-zA-Z0-9\\+\\#\\.]{2,}\\b\", jd_text.lower()))\n",
    "    resume_keywords = set(re.findall(r\"\\b[a-zA-Z0-9\\+\\#\\.]{2,}\\b\", resume_text.lower()))\n",
    "    matched_keywords = jd_keywords.intersection(resume_keywords)\n",
    "    unmatched_keywords = jd_keywords - resume_keywords\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nüü¶ JD Keywords:\", sorted(list(jd_keywords)))\n",
    "        print(\"üü© Matched Keywords:\", sorted(list(matched_keywords)))\n",
    "        print(\"üü• Unmatched Keywords:\", sorted(list(unmatched_keywords)))\n",
    "\n",
    "    score = len(matched_keywords) / len(jd_keywords) if jd_keywords else 0.0\n",
    "    return score\n",
    "\n",
    "def section_overlap_score(jd_sections, resume_sections, verbose=True):\n",
    "    overlap = 0\n",
    "    total = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nüìò Section Overlap Debug:\")\n",
    "\n",
    "    for key in ['required_skills', 'responsibilities', 'qualifications']:\n",
    "        jd_text = jd_sections.get(key, '').lower()\n",
    "        res_key = 'skills' if key == 'required_skills' else (\n",
    "            'experience' if key == 'responsibilities' else 'education')\n",
    "        res_text = resume_sections.get(res_key, '').lower()\n",
    "\n",
    "        jd_words = set(jd_text.split())\n",
    "        res_words = set(res_text.split())\n",
    "\n",
    "        matched = jd_words.intersection(res_words)\n",
    "        section_score = len(matched) / len(jd_words) if jd_words else 0.0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nüîπ JD Section: {key} | Resume Section: {res_key}\")\n",
    "            print(f\"JD Words ({len(jd_words)}): {sorted(list(jd_words))[:20]}...\")\n",
    "            print(f\"Resume Words ({len(res_words)}): {sorted(list(res_words))[:20]}...\")\n",
    "            print(f\"Matched ({len(matched)}): {sorted(list(matched))[:20]}...\")\n",
    "            print(f\"Section Score: {round(section_score, 4)}\")\n",
    "\n",
    "        overlap += section_score\n",
    "        total += 1\n",
    "\n",
    "    return overlap / total if total else 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_score(jd_text, resume_text):\n",
    "    jd_sections = extract_sections(jd_text, is_resume=False)\n",
    "    resume_sections = extract_sections(resume_text, is_resume=True)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC MATCHING =================\")\n",
    "    sem_score = semantic_score(jd_sections, resume_sections, verbose=True)\n",
    "\n",
    "    print(\"\\n================ KEYWORD MATCHING ==================\")\n",
    "    key_score = keyword_score(jd_text, resume_text, verbose=True)\n",
    "\n",
    "    print(\"\\n================ SECTION OVERLAP ===================\")\n",
    "    sect_score = section_overlap_score(jd_sections, resume_sections, verbose=True)\n",
    "\n",
    "    final = (0.6 * sem_score) + (0.2 * key_score) + (0.2 * sect_score)\n",
    "    print(\"\\n================ FINAL SCORE =======================\")\n",
    "    return {\n",
    "        \"semantic_score\": round(sem_score, 4),\n",
    "        \"keyword_score\": round(key_score, 4),\n",
    "        \"section_overlap_score\": round(sect_score, 4),\n",
    "        \"final_score\": round(final, 4)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ SEMANTIC MATCHING =================\n",
      "\n",
      "üìò Semantic Similarity by Section:\n",
      "\n",
      "üîπ JD Section: required_skills ‚Üî Resume Section: skills\n",
      "Similarity Score: 0.4013\n",
      "\n",
      "üîπ JD Section: responsibilities ‚Üî Resume Section: experience\n",
      "Similarity Score: 0.471\n",
      "\n",
      "üîπ JD Section: qualifications ‚Üî Resume Section: education\n",
      "Similarity Score: 0.4959\n",
      "\n",
      "‚úÖ Final Semantic Score (average): 0.4561\n",
      "\n",
      "================ KEYWORD MATCHING ==================\n",
      "\n",
      "üü¶ JD Keywords: ['ability', 'about', 'across', 'actionable', 'activating', 'analysis', 'analysts', 'analytical', 'analytics', 'and', 'are', 'as', 'assist', 'attention', 'bachelor', 'banks', 'between', 'big', 'brand', 'brands', 'build', 'building', 'business', 'but', 'by', 'capabilities', 'cases', 'choice', 'clarity', 'collaborate', 'collaboration', 'collaboratively', 'companies', 'complete', 'complex', 'compliance', 'computer', 'concepts', 'connected', 'consumer', 'context', 'contribute', 'contributing', 'cross', 'currently', 'customer', 'dashboards', 'data', 'database', 'deepening', 'degree', 'design', 'designing', 'detail', 'developing', 'driven', 'enabling', 'enduring', 'engagement', 'engineering', 'engineers', 'enhance', 'ensuring', 'environment', 'ethics', 'etl', 'evolving', 'excellent', 'experience', 'explore', 'fast', 'field', 'first', 'flexibility', 'for', 'forefront', 'foundational', 'from', 'functional', 'giants', 'global', 'groundbreaking', 'growing', 'hadoop', 'healthcare', 'hundreds', 'iconic', 'ideal', 'identity', 'implementation', 'improve', 'in', 'infrastructure', 'initiatives', 'innovative', 'innovators', 'insights', 'integrity', 'intern', 'internship', 'into', 'is', 'its', 'join', 'kafka', 'leader', 'leaders', 'liveramp', 'lives', 'looking', 'loyalty', 'maintaining', 'master', 'maximizing', 'mindset', 'modeling', 'models', 'monitoring', 'most', 'network', 'new', 'non', 'not', 'of', 'offers', 'on', 'optimizing', 'or', 'organizations', 'oriented', 'our', 'paced', 'partners', 'partnerships', 'party', 'passionate', 'pipelines', 'platform', 'platforms', 'plus', 'precious', 'premier', 'privacy', 'problem', 'problems', 'processes', 'processing', 'proficiency', 'protecting', 'pursuing', 'python', 'quality', 'range', 'rapidly', 'raw', 'related', 'relational', 'reporting', 'required', 'requirements', 'retailers', 'role', 'science', 'setting', 'skills', 'solutions', 'solving', 'spark', 'sql', 'standard', 'staying', 'strong', 'students', 'such', 'support', 'systems', 'team', 'teams', 'tech', 'technical', 'technologies', 'the', 'their', 'them', 'this', 'to', 'tools', 'top', 'transform', 'translate', 'trust', 'turn', 'understand', 'understanding', 'unmatched', 'use', 'value', 'view', 'wherever', 'while', 'who', 'widest', 'will', 'with', 'within', 'work', 'world', 'you']\n",
      "üü© Matched Keywords: ['analytical', 'analytics', 'and', 'bachelor', 'big', 'business', 'by', 'computer', 'customer', 'dashboards', 'data', 'driven', 'enabling', 'etl', 'experience', 'fast', 'field', 'for', 'from', 'in', 'integrity', 'master', 'models', 'monitoring', 'non', 'of', 'on', 'pipelines', 'platforms', 'processing', 'python', 'quality', 'science', 'skills', 'solutions', 'spark', 'sql', 'technical', 'the', 'to', 'tools', 'turn', 'while', 'with']\n",
      "üü• Unmatched Keywords: ['ability', 'about', 'across', 'actionable', 'activating', 'analysis', 'analysts', 'are', 'as', 'assist', 'attention', 'banks', 'between', 'brand', 'brands', 'build', 'building', 'but', 'capabilities', 'cases', 'choice', 'clarity', 'collaborate', 'collaboration', 'collaboratively', 'companies', 'complete', 'complex', 'compliance', 'concepts', 'connected', 'consumer', 'context', 'contribute', 'contributing', 'cross', 'currently', 'database', 'deepening', 'degree', 'design', 'designing', 'detail', 'developing', 'enduring', 'engagement', 'engineering', 'engineers', 'enhance', 'ensuring', 'environment', 'ethics', 'evolving', 'excellent', 'explore', 'first', 'flexibility', 'forefront', 'foundational', 'functional', 'giants', 'global', 'groundbreaking', 'growing', 'hadoop', 'healthcare', 'hundreds', 'iconic', 'ideal', 'identity', 'implementation', 'improve', 'infrastructure', 'initiatives', 'innovative', 'innovators', 'insights', 'intern', 'internship', 'into', 'is', 'its', 'join', 'kafka', 'leader', 'leaders', 'liveramp', 'lives', 'looking', 'loyalty', 'maintaining', 'maximizing', 'mindset', 'modeling', 'most', 'network', 'new', 'not', 'offers', 'optimizing', 'or', 'organizations', 'oriented', 'our', 'paced', 'partners', 'partnerships', 'party', 'passionate', 'platform', 'plus', 'precious', 'premier', 'privacy', 'problem', 'problems', 'processes', 'proficiency', 'protecting', 'pursuing', 'range', 'rapidly', 'raw', 'related', 'relational', 'reporting', 'required', 'requirements', 'retailers', 'role', 'setting', 'solving', 'standard', 'staying', 'strong', 'students', 'such', 'support', 'systems', 'team', 'teams', 'tech', 'technologies', 'their', 'them', 'this', 'top', 'transform', 'translate', 'trust', 'understand', 'understanding', 'unmatched', 'use', 'value', 'view', 'wherever', 'who', 'widest', 'will', 'within', 'work', 'world', 'you']\n",
      "\n",
      "================ SECTION OVERLAP ===================\n",
      "\n",
      "üìò Section Overlap Debug:\n",
      "\n",
      "üîπ JD Section: required_skills | Resume Section: skills\n",
      "JD Words (83): ['&', '.', '1.', '2.', '3.', '4.', 'a', 'about', 'actionable', 'analysts,', 'analytics', 'analytics,', 'and', 'are', 'assist', 'build', 'business', 'collaborate', 'complex', 'contributing']...\n",
      "Resume Words (322): ['1179,', '200m,', '2018,', '2021,', '2022,', '2024,', '2025,', '206,', '20m,', '25,', '2x,', '3.2,', '3.8,', '30,', '35,', '4.0,', '40,', '50', '50,', '60,']...\n",
      "Matched (3): ['analytics,', 'pipelines,', 'solutions,']...\n",
      "Section Score: 0.0361\n",
      "\n",
      "üîπ JD Section: responsibilities | Resume Section: experience\n",
      "JD Words (225): ['&', '(relational', '(such', '1.', '2.', '3.', '4.', '5.', '6.', 'a', 'ability', 'about', 'across', 'actionable', 'activating', 'analysis.', 'analysts,', 'analytical', 'analytics', 'analytics,']...\n",
      "Resume Words (99): ['(star/snowflake', ',', '.', '200m+', '2022', '2024', '20m+', '25%,', '2x,', '30%', '30%.', '35%', '40%', '40%,', '50%', 'a', 'accuracy', 'ad-hoc', 'airflow,', 'analyst']...\n",
      "Matched (19): ['a', 'analytical', 'and', 'business', 'by', 'data', 'enabling', 'etl', 'for', 'integrity', 'models', 'monitoring', 'of', 'on', 'pipelines', 'processing', 'quality', 'sql', 'the']...\n",
      "Section Score: 0.0844\n",
      "\n",
      "üîπ JD Section: qualifications | Resume Section: education\n",
      "JD Words (225): ['&', '(relational', '(such', '1.', '2.', '3.', '4.', '5.', '6.', 'a', 'ability', 'about', 'across', 'actionable', 'activating', 'analysis.', 'analysts,', 'analytical', 'analytics', 'analytics,']...\n",
      "Resume Words (333): ['&', '(emr,', '(large', '(star/snowflake', '+', ',', '.', '/gtb|hackathon,', '/gtb|nlp,', '/gtb|python,', '200m+', '2018', '2021', '2022', '2024', '2025', '20m+', '25%,', '2x,', '3.2']...\n",
      "Matched (37): ['&', 'a', 'analytical', 'analytics,', 'and', 'big', 'business', 'by', 'computer', 'customer', 'dashboards', 'data', 'data-driven', 'enabling', 'etl', 'experience', 'for', 'from', 'in', 'integrity']...\n",
      "Section Score: 0.1644\n",
      "\n",
      "================ FINAL SCORE =======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'semantic_score': 0.4561,\n",
       " 'keyword_score': 0.2115,\n",
       " 'section_overlap_score': 0.095,\n",
       " 'final_score': 0.335}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this with your file paths\n",
    "resume_path = \"resumes/Dhruvraj_resume_May18.pdf\"  # Replace if renamed\n",
    "jd_path = \"JDs/Data_engineering_intern_LiveRamp.txt\"                # Upload your JD as a .txt file\n",
    "\n",
    "resume_text = extract_text_from_pdf(resume_path)\n",
    "jd_text = extract_text_from_txt(jd_path)\n",
    "\n",
    "result = final_score(jd_text, resume_text)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
