{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    sections = {\n",
    "        \"skills\": \"\",\n",
    "        \"experience\": \"\",\n",
    "        \"projects\": \"\",\n",
    "        \"other\": \"\"\n",
    "    }\n",
    "    current = \"other\"\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        l = line.lower()\n",
    "        if \"skill\" in l:\n",
    "            current = \"skills\"\n",
    "        elif \"experience\" in l:\n",
    "            current = \"experience\"\n",
    "        elif \"project\" in l:\n",
    "            current = \"projects\"\n",
    "        sections[current] += line + \" \"\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_score(resume_sections, jd_text):\n",
    "    jd_emb = model.encode(jd_text, convert_to_tensor=True)\n",
    "    weights = {\n",
    "        \"skills\": 0.1,\n",
    "        \"experience\": 0.6,\n",
    "        \"projects\": 0.3\n",
    "    }\n",
    "\n",
    "    score = 0.0\n",
    "    for sec, text in resume_sections.items():\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "        emb = model.encode(text, convert_to_tensor=True)\n",
    "        sim = util.pytorch_cos_sim(emb, jd_emb).item()\n",
    "        score += weights.get(sec, 0.0) * sim\n",
    "    return round(score * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dir = \"resumes/\"\n",
    "# jd_path = \"JDs/AI_intern_Armada.txt\"\n",
    "jd_path = \"JDs/Data_engineering_intern_LiveRamp.txt\"\n",
    "\n",
    "with open(jd_path, 'r', encoding='utf-8') as f:\n",
    "    jd_text = f.read()\n",
    "\n",
    "results = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(resume_dir):\n",
    "    if not file.endswith(\".pdf\"):\n",
    "        continue\n",
    "    fpath = os.path.join(resume_dir, file)\n",
    "    text = extract_text_from_pdf(fpath)\n",
    "    sections = extract_sections(text)\n",
    "    score = weighted_score(sections, jd_text)\n",
    "    results.append({\"Resume\": file, \"Weighted Score\": score})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Weighted Score\", ascending=False)\n",
    "results_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume</th>\n",
       "      <th>Weighted Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhruvraj_resume_May18.pdf</td>\n",
       "      <td>42.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dhruvraj_resume_MSDS.pdf</td>\n",
       "      <td>42.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dhruvraj_resume_USHunger.pdf</td>\n",
       "      <td>38.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dhruvraj_resume_Mar11.pdf</td>\n",
       "      <td>38.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dhruvraj_Resume_intern_rocket.pdf</td>\n",
       "      <td>38.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dhruvraj_resume_image_analytics.pdf</td>\n",
       "      <td>8.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Resume  Weighted Score\n",
       "0            Dhruvraj_resume_May18.pdf           42.59\n",
       "1             Dhruvraj_resume_MSDS.pdf           42.23\n",
       "2         Dhruvraj_resume_USHunger.pdf           38.99\n",
       "3            Dhruvraj_resume_Mar11.pdf           38.17\n",
       "4    Dhruvraj_Resume_intern_rocket.pdf           38.17\n",
       "5  Dhruvraj_resume_image_analytics.pdf            8.08"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## latest\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from math import ceil\n",
    "\n",
    "def summarize_batch_with_ollama(jd_text, resume_batch):\n",
    "    batch_prompts = \"\"\n",
    "    for idx, resume_text in enumerate(resume_batch, start=1):\n",
    "        batch_prompts += f\"\\nResume {idx}:\\n\\\"\\\"\\\"{resume_text[:2000]}\\\"\\\"\\\"\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant helping recruiters evaluate candidates by comparing their resumes with a job description. For each resume, provide:\n",
    "\n",
    "1. A 40-word summary of how the resume aligns with the JD.\n",
    "2. A bullet list of 5 key relevant skills.\n",
    "3. A one-line highlight of the candidate‚Äôs relevant experience.\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"{jd_text}\\\"\\\"\\\"\n",
    "\n",
    "{batch_prompts}\n",
    "\n",
    "Format output as:\n",
    "\n",
    "Resume 1:\n",
    "Summary: ...\n",
    "Relevant Skills:\n",
    "- ...\n",
    "Main Highlight: ...\n",
    "\n",
    "Resume 2:\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3.2',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_resumes_ollama(jd_text, resume_texts, output_path=\"resume_summaries.txt\"):\n",
    "    batch_size = 3\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(resume_texts), batch_size):\n",
    "        batch = resume_texts[i:i+batch_size]\n",
    "        result = summarize_batch_with_ollama(jd_text, batch)\n",
    "        results.append(result)\n",
    "\n",
    "    all_summary_text = \"\\n\\n\".join(results)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_summary_text)\n",
    "\n",
    "    print(f\"‚úÖ Summaries saved to {output_path}\")\n",
    "    return all_summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summaries saved to summary_ollama_output.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here is the evaluation of the three resumes against the job description:\\n\\n**Resume 1**\\n\\nSummary: The resume aligns with the JD as it showcases Dhruvraj's experience in data pipeline development, ETL processes, and analytics solutions. The candidate has worked on various projects that demonstrate their proficiency in Python, SQL, and big data tools.\\n\\nRelevant Skills:\\n- Data pipeline development\\n- ETL processes\\n- Analytics solutions\\n- Big data tools (PySpark, Hadoop, Kafka)\\n- Cloud computing (AWS Suite)\\n\\nMain Highlight: Dhruvraj's experience as a Data Analyst at Draup Business Solutions Bangalore, where he designed and deployed high-performance ETL pipelines using PySpark and SQL on AWS EMR, improving data integrity by 35%.\\n\\n**Resume 2**\\n\\nSummary: The resume aligns with the JD as it highlights Dhruvraj's expertise in machine learning, deep learning, and big data. The candidate has worked on various projects that demonstrate their proficiency in machine learning frameworks like Scikit-learn, TensorFlow, and PyTorch.\\n\\nRelevant Skills:\\n- Machine learning\\n- Deep learning\\n- Big data tools (Spark, Hadoop, Kafka)\\n- ETL automation pipelines\\n- Data visualization\\n\\nMain Highlight: Dhruvraj's experience working on a personalized research assistant project using RAG and LangChain, which demonstrates their ability to build AI applications.\\n\\n**Resume 3**\\n\\nSummary: The resume aligns with the JD as it showcases Dhruvraj's experience in data analysis, ETL processes, and analytics solutions. The candidate has worked on various projects that demonstrate their proficiency in Python, SAS, and big data tools.\\n\\nRelevant Skills:\\n- Data analysis\\n- ETL processes\\n- Analytics solutions\\n- Big data tools (PySpark, SQL)\\n- Cloud computing (AWS Suite)\\n\\nMain Highlight: Dhruvraj's experience as a Student Research Assistant at Texas A&M University College Station, where they conducted statistical analysis on Medicaid data using Python and SAS.\\n\\nHere are the outputs for each resume:\\n\\n\\nResume 1:\\nSummary: Aligns with JD requirements in data analysis, ETL processes, and data visualization, showcasing proficiency in Python, SQL, and big data tools like PySpark.\\n\\nRelevant Skills:\\n- Data validation frameworks\\n- ETL processes\\n- Big data tools (PySpark)\\n- Data mining and wrangling pipelines\\n- Dashboard development\\n\\nMain Highlight: Implemented scalable and robust ETL data validation frameworks utilizing PySpark and SQL, resulting in a 35% improvement in end-to-end data integrity.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_paths = [\n",
    "    \"resumes/Dhruvraj_resume_May18.pdf\",\n",
    "    \"resumes/Dhruvraj_resume_MSDS.pdf\",\n",
    "    \"resumes/Dhruvraj_resume_USHunger.pdf\",\n",
    "    \"resumes/Dhruvraj_Resume_intern_rocket.pdf\"\n",
    "]\n",
    "\n",
    "resume_texts = [extract_text_from_pdf(path) for path in resume_paths]\n",
    "jd_text = open(\"JDs/Data_engineering_intern_LiveRamp.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "summarize_resumes_ollama(jd_text, resume_texts, \"summary_ollama_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below code is with the cross encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Dhruvraj_resume_Mar11.pdf\n",
      "Processing: Dhruvraj_Resume_intern_rocket.pdf\n",
      "Processing: Dhruvraj_resume_May18.pdf\n",
      "Processing: Shefali_Bisht_Resume_BIE.pdf\n",
      "Processing: Dhruvraj_resume_image_analytics.pdf\n",
      "Processing: Dhruvraj_resume_USHunger.pdf\n",
      "Processing: Dhruvraj_resume_MSDS.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume</th>\n",
       "      <th>BERT Score</th>\n",
       "      <th>Keyword Overlap</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shefali_Bisht_Resume_BIE.pdf</td>\n",
       "      <td>44.48</td>\n",
       "      <td>30.77</td>\n",
       "      <td>41.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dhruvraj_resume_Mar11.pdf</td>\n",
       "      <td>44.49</td>\n",
       "      <td>23.59</td>\n",
       "      <td>40.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dhruvraj_resume_MSDS.pdf</td>\n",
       "      <td>43.80</td>\n",
       "      <td>21.03</td>\n",
       "      <td>39.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dhruvraj_resume_May18.pdf</td>\n",
       "      <td>43.07</td>\n",
       "      <td>18.97</td>\n",
       "      <td>38.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dhruvraj_resume_USHunger.pdf</td>\n",
       "      <td>42.56</td>\n",
       "      <td>17.95</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dhruvraj_Resume_intern_rocket.pdf</td>\n",
       "      <td>42.26</td>\n",
       "      <td>16.92</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dhruvraj_resume_image_analytics.pdf</td>\n",
       "      <td>10.63</td>\n",
       "      <td>12.31</td>\n",
       "      <td>10.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Resume  BERT Score  Keyword Overlap  \\\n",
       "0         Shefali_Bisht_Resume_BIE.pdf       44.48            30.77   \n",
       "1            Dhruvraj_resume_Mar11.pdf       44.49            23.59   \n",
       "2             Dhruvraj_resume_MSDS.pdf       43.80            21.03   \n",
       "3            Dhruvraj_resume_May18.pdf       43.07            18.97   \n",
       "4         Dhruvraj_resume_USHunger.pdf       42.56            17.95   \n",
       "5    Dhruvraj_Resume_intern_rocket.pdf       42.26            16.92   \n",
       "6  Dhruvraj_resume_image_analytics.pdf       10.63            12.31   \n",
       "\n",
       "   Combined Score  \n",
       "0           41.74  \n",
       "1           40.31  \n",
       "2           39.25  \n",
       "3           38.25  \n",
       "4           37.64  \n",
       "5           37.19  \n",
       "6           10.97  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Section name mappings\n",
    "SECTION_MAP = {\n",
    "    \"skills\": [\"skills\", \"technical skills\", \"skill set\"],\n",
    "    \"experience\": [\"experience\", \"professional experience\", \"technical experience\", \"work experience\"],\n",
    "    \"projects\": [\"projects\", \"academic projects\", \"personal projects\"]\n",
    "}\n",
    "\n",
    "# Weights for section-wise scoring\n",
    "SECTION_WEIGHTS = {\n",
    "    \"skills\": 0.1,\n",
    "    \"experience\": 0.6,\n",
    "    \"projects\": 0.2,\n",
    "    \"other\": 0.1\n",
    "}\n",
    "\n",
    "# Stopwords to filter out for keyword overlap\n",
    "STOPWORDS = set([\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"as\", \"is\", \"are\", \"was\", \"were\", \"to\", \"of\", \"in\", \"on\", \"for\", \"by\", \"with\",\n",
    "    \"that\", \"this\", \"at\", \"from\", \"it\", \"be\", \"which\", \"but\", \"if\", \"they\", \"has\", \"have\", \"had\", \"will\", \"would\",\n",
    "    \"can\", \"could\", \"their\", \"its\", \"about\"\n",
    "])\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    return text\n",
    "\n",
    "def clean_line(line):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', line).strip().lower()\n",
    "\n",
    "def extract_sections(text):\n",
    "    sections = {key: \"\" for key in SECTION_WEIGHTS.keys()}\n",
    "    current = \"other\"\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line_clean = clean_line(line)\n",
    "        if not line_clean:\n",
    "            continue\n",
    "        for key, aliases in SECTION_MAP.items():\n",
    "            if any(alias in line_clean for alias in aliases):\n",
    "                current = key\n",
    "                break\n",
    "        sections[current] += line + \" \"\n",
    "    return sections\n",
    "\n",
    "def get_keyword_overlap_score(jd_text, resume_text):\n",
    "    jd_keywords = set(w for w in re.findall(r'\\w+', jd_text.lower()) if w not in STOPWORDS)\n",
    "    resume_words = set(w for w in re.findall(r'\\w+', resume_text.lower()) if w not in STOPWORDS)\n",
    "    common = jd_keywords.intersection(resume_words)\n",
    "    return round(len(common) / max(len(jd_keywords), 1), 4)  # normalized %\n",
    "\n",
    "def compute_scores(text, sections, jd_text):\n",
    "    jd_emb = bert_model.encode(jd_text, convert_to_tensor=True)\n",
    "    bert_score = 0.0\n",
    "\n",
    "    for sec, content in sections.items():\n",
    "        if content.strip() == \"\":\n",
    "            continue\n",
    "        emb = bert_model.encode(content, convert_to_tensor=True)\n",
    "        sim = util.pytorch_cos_sim(emb, jd_emb).item()\n",
    "        bert_score += SECTION_WEIGHTS.get(sec, 0.0) * sim\n",
    "\n",
    "    # Keyword Overlap\n",
    "    overlap_score = get_keyword_overlap_score(jd_text, text)\n",
    "\n",
    "    # Final Combined Score: 80% BERT + 20% Overlap\n",
    "    final_score = 0.8 * bert_score + 0.2 * overlap_score\n",
    "\n",
    "    return {\n",
    "        \"BERT Score\": round(bert_score * 100, 2),\n",
    "        \"Keyword Overlap\": round(overlap_score * 100, 2),\n",
    "        \"Combined Score\": round(final_score * 100, 2)\n",
    "    }\n",
    "\n",
    "# Paths\n",
    "resume_dir = \"resumes/\"\n",
    "jd_path = \"JDs/Data_engineering_intern_LiveRamp.txt\"\n",
    "\n",
    "with open(jd_path, 'r', encoding='utf-8') as f:\n",
    "    jd_text = f.read()\n",
    "\n",
    "results = []\n",
    "\n",
    "for file in os.listdir(resume_dir):\n",
    "    if not file.endswith(\".pdf\"):\n",
    "        continue\n",
    "    print(f\"Processing: {file}\")\n",
    "    fpath = os.path.join(resume_dir, file)\n",
    "    resume_text = extract_text_from_pdf(fpath)\n",
    "    resume_sections = extract_sections(resume_text)\n",
    "    scores = compute_scores(resume_text, resume_sections, jd_text)\n",
    "    results.append({\"Resume\": file, **scores})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Combined Score\", ascending=False)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Score of BERT, Similarity score by LLAMA3.2 and keyword overlap score combined:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dhruv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Dhruvraj_resume_Mar11.pdf\n",
      "‚Üí Dhruvraj_Resume_intern_rocket.pdf\n",
      "‚Üí Dhruvraj_resume_May18.pdf\n",
      "‚Üí Shefali_Bisht_Resume_BIE.pdf\n",
      "‚Üí Dhruvraj_resume_image_analytics.pdf\n",
      "‚Üí Dhruvraj_resume_USHunger.pdf\n",
      "‚Üí Dhruvraj_resume_MSDS.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume</th>\n",
       "      <th>BERT Score</th>\n",
       "      <th>LLaMA Score</th>\n",
       "      <th>Keyword Overlap</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhruvraj_resume_May18.pdf</td>\n",
       "      <td>43.07</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.22</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dhruvraj_resume_USHunger.pdf</td>\n",
       "      <td>42.56</td>\n",
       "      <td>80.0</td>\n",
       "      <td>18.54</td>\n",
       "      <td>48.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dhruvraj_resume_Mar11.pdf</td>\n",
       "      <td>44.49</td>\n",
       "      <td>70.0</td>\n",
       "      <td>24.16</td>\n",
       "      <td>47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dhruvraj_resume_MSDS.pdf</td>\n",
       "      <td>43.80</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21.91</td>\n",
       "      <td>47.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shefali_Bisht_Resume_BIE.pdf</td>\n",
       "      <td>44.48</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.90</td>\n",
       "      <td>43.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dhruvraj_Resume_intern_rocket.pdf</td>\n",
       "      <td>42.26</td>\n",
       "      <td>50.0</td>\n",
       "      <td>17.98</td>\n",
       "      <td>40.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dhruvraj_resume_image_analytics.pdf</td>\n",
       "      <td>19.69</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>25.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Resume  BERT Score  LLaMA Score  \\\n",
       "0            Dhruvraj_resume_May18.pdf       43.07        100.0   \n",
       "1         Dhruvraj_resume_USHunger.pdf       42.56         80.0   \n",
       "2            Dhruvraj_resume_Mar11.pdf       44.49         70.0   \n",
       "3             Dhruvraj_resume_MSDS.pdf       43.80         70.0   \n",
       "4         Shefali_Bisht_Resume_BIE.pdf       44.48         50.0   \n",
       "5    Dhruvraj_Resume_intern_rocket.pdf       42.26         50.0   \n",
       "6  Dhruvraj_resume_image_analytics.pdf       19.69         50.0   \n",
       "\n",
       "   Keyword Overlap  Combined Score  \n",
       "0            20.22           53.88  \n",
       "1            18.54           48.32  \n",
       "2            24.16           47.82  \n",
       "3            21.91           47.07  \n",
       "4            30.90           43.82  \n",
       "5            17.98           40.55  \n",
       "6            10.67           25.91  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, PyPDF2, pandas as pd, nltk, torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# ‚îÄ‚îÄ one-time NLTK download ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ‚îÄ‚îÄ models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "llm        = ChatOllama(model='llama3.2')\n",
    "\n",
    "# ‚îÄ‚îÄ section headers & base weights (before dynamic re-allocation) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SECTION_MAP = {\n",
    "    \"skills\"    : [\"skills\", \"technical skills\", \"skill set\"],\n",
    "    \"experience\": [\"experience\", \"professional experience\", \"technical experience\", \"work experience\"],\n",
    "    \"projects\"  : [\"projects\", \"academic projects\", \"personal projects\"]\n",
    "}\n",
    "BASE_WEIGHTS = {          # will be copied & adjusted per-resume\n",
    "    \"skills\"    : 0.10,\n",
    "    \"experience\": 0.60,\n",
    "    \"projects\"  : 0.20,\n",
    "    \"other\"     : 0.10\n",
    "}\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def extract_text_from_pdf(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "clean = lambda line: re.sub(r'[^A-Za-z0-9\\s]', '', line).strip().lower()\n",
    "\n",
    "def extract_sections(text: str) -> dict:\n",
    "    sections, current = {k: \"\" for k in BASE_WEIGHTS}, \"other\"\n",
    "    for raw in text.splitlines():\n",
    "        line = clean(raw)\n",
    "        if not line: continue\n",
    "        for key, aliases in SECTION_MAP.items():\n",
    "            if any(alias in line for alias in aliases):\n",
    "                current = key; break\n",
    "        sections[current] += raw + \" \"\n",
    "    return sections\n",
    "\n",
    "def keyword_overlap(jd, doc):\n",
    "    jd_kw  = {w for w in re.findall(r'\\w+', jd.lower())  if w not in STOPWORDS}\n",
    "    doc_kw = {w for w in re.findall(r'\\w+', doc.lower()) if w not in STOPWORDS}\n",
    "    return len(jd_kw & doc_kw) / max(len(jd_kw), 1)\n",
    "\n",
    "def llama_similarity(jd, exp_proj):\n",
    "    prompt = (\n",
    "        \"You're a hiring assistant. Based only on the experience & projects below, \"\n",
    "        \"rate the candidate's relevance to the job description on a scale 0-1. \"\n",
    "        \"Respond with **only** the number.\\n\\n\"\n",
    "        f\"Job Description:\\n{jd}\\n\\nExperience & Projects:\\n{exp_proj}\"\n",
    "    )\n",
    "    reply = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    match = re.search(r\"(?:0?\\.\\d+|1(?:\\.0+)?)\", reply)\n",
    "    return float(match.group()) if match else 0.0\n",
    "\n",
    "def dynamic_weights(sec_dict):\n",
    "    \"\"\"Return a weight dict that shifts EXP‚ÜíPROJ or PROJ‚ÜíEXP if one is empty.\"\"\"\n",
    "    w = BASE_WEIGHTS.copy()\n",
    "    has_exp = sec_dict[\"experience\"].strip() != \"\"\n",
    "    has_prj = sec_dict[\"projects\"].strip()   != \"\"\n",
    "    if not has_exp and has_prj:          # move 0.60 to projects\n",
    "        w[\"projects\"] += w[\"experience\"]; w[\"experience\"] = 0.0\n",
    "    elif has_exp and not has_prj:        # move 0.20 to experience\n",
    "        w[\"experience\"] += w[\"projects\"]; w[\"projects\"]  = 0.0\n",
    "    return w\n",
    "\n",
    "def score_resume(fname, full_txt, secs, jd_txt):\n",
    "    # ----- BERT section score -------------------------------------------------\n",
    "    w = dynamic_weights(secs)\n",
    "    jd_emb, bert_total = bert_model.encode(jd_txt, convert_to_tensor=True), 0.0\n",
    "    for sec, txt in secs.items():\n",
    "        if not txt.strip() or w[sec]==0: continue\n",
    "        emb  = bert_model.encode(txt, convert_to_tensor=True)\n",
    "        sim  = util.pytorch_cos_sim(emb, jd_emb).item()\n",
    "        bert_total += w[sec] * sim      # weighted sum (still 0-1 range)\n",
    "\n",
    "    # ----- keyword overlap ----------------------------------------------------\n",
    "    kw_overlap = keyword_overlap(jd_txt, full_txt)\n",
    "\n",
    "    # ----- LLaMA similarity (experience + projects) ---------------------------\n",
    "    exp_proj   = secs[\"experience\"] + \"\\n\" + secs[\"projects\"]\n",
    "    llama_sc   = llama_similarity(jd_txt, exp_proj)\n",
    "\n",
    "    # ----- final weighted combo ----------------------------------------------\n",
    "    final = 0.60 * bert_total + 0.25 * llama_sc + 0.15 * kw_overlap\n",
    "    return dict(\n",
    "        Resume=fname,\n",
    "        **{ \"BERT Score\": round(bert_total*100,2),\n",
    "            \"LLaMA Score\": round(llama_sc*100,2),\n",
    "            \"Keyword Overlap\": round(kw_overlap*100,2),\n",
    "            \"Combined Score\": round(final*100,2) }\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ run pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "RES_DIR, JD_FILE = \"resumes/\", \"JDs/Data_engineering_intern_LiveRamp.txt\"\n",
    "jd_text = open(JD_FILE, encoding='utf-8').read()\n",
    "\n",
    "results = []\n",
    "for fn in os.listdir(RES_DIR):\n",
    "    if not fn.endswith(\".pdf\"): continue\n",
    "    print(\"‚Üí\", fn)\n",
    "    txt   = extract_text_from_pdf(os.path.join(RES_DIR, fn))\n",
    "    secs  = extract_sections(txt)\n",
    "    results.append(score_resume(fn, txt, secs, jd_text))\n",
    "\n",
    "df = pd.DataFrame(results).sort_values(\"Combined Score\", ascending=False).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score with BERT, LLAMA3.2 and keyword overlap with clean JD text for scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dhruv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Filtered Job Description used for scoring:\n",
      "\n",
      "What You'll Do (Key Responsibilities)\n",
      "\n",
      "Assist in building, training, and fine-tuning machine learning models.\n",
      "Conduct research on AI trends, tools, and techniques.\n",
      "Work with large datasets for data preprocessing, cleaning, and feature engineering.\n",
      "Optimize and evaluate model performance using various metrics.\n",
      "Support AI team members in deploying and integrating models into applications.\n",
      "Write and document scripts, workflows, and processes.\n",
      "Collaborate with cross-functional teams, including data engineers and software developers.\n",
      "Stay updated on the latest AI advancements and research papers.\n",
      "\n",
      "Required Qualifications\n",
      "\n",
      "Pursuing or recently completed a degree in Computer Science, Data Science, Artificial Intelligence, or a related field.\n",
      "Familiarity with programming languages such as Python, R, or Java.\n",
      "Knowledge of AI/ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\n",
      "Experience with data manipulation using Pandas, NumPy, and SQL.\n",
      "Understanding of deep learning, NLP, or computer vision is a plus.\n",
      "Strong problem-solving and analytical skills.\n",
      "Ability to work independently and in a team-oriented environment.\n",
      "\n",
      "Compensation\n",
      "\n",
      "$30‚Äî$30 USD\n",
      "\n",
      "You're a Great Fit if You're\n",
      "\n",
      "A go-getter with a growth mindset. You're intellectually curious, have strong business acumen, and actively seek opportunities to build relevant skills and knowledge\n",
      "A detail-oriented problem-solver. You can independently gather information, solve problems efficiently, and deliver results with a \"get-it-done\" attitude\n",
      "Thrive in a fast-paced environment. You're energized by an entrepreneurial spirit, capable of working quickly, and excited to contribute to a growing company\n",
      "A collaborative team player. You focus on business success and are motivated by team accomplishment vs personal agenda\n",
      "Highly organized and results-driven. Strong prioritization skills and a dedicated work ethic are essential for you\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚Üí Dhruvraj_resume_Mar11.pdf\n",
      "‚Üí Dhruvraj_Resume_intern_rocket.pdf\n",
      "‚Üí Dhruvraj_resume_May18.pdf\n",
      "‚Üí Shefali_Bisht_Resume_BIE.pdf\n",
      "‚Üí Dhruvraj_resume_image_analytics.pdf\n",
      "‚Üí Dhruvraj_resume_USHunger.pdf\n",
      "‚Üí Dhruvraj_resume_MSDS.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume</th>\n",
       "      <th>BERT Score</th>\n",
       "      <th>LLaMA Score</th>\n",
       "      <th>Keyword Overlap</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dhruvraj_resume_MSDS.pdf</td>\n",
       "      <td>38.83</td>\n",
       "      <td>80.0</td>\n",
       "      <td>28.39</td>\n",
       "      <td>47.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dhruvraj_resume_May18.pdf</td>\n",
       "      <td>28.14</td>\n",
       "      <td>95.0</td>\n",
       "      <td>23.23</td>\n",
       "      <td>44.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dhruvraj_resume_USHunger.pdf</td>\n",
       "      <td>25.46</td>\n",
       "      <td>80.0</td>\n",
       "      <td>23.23</td>\n",
       "      <td>38.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dhruvraj_Resume_intern_rocket.pdf</td>\n",
       "      <td>28.24</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21.94</td>\n",
       "      <td>37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shefali_Bisht_Resume_BIE.pdf</td>\n",
       "      <td>35.62</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25.81</td>\n",
       "      <td>37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dhruvraj_resume_Mar11.pdf</td>\n",
       "      <td>29.52</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.87</td>\n",
       "      <td>36.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dhruvraj_resume_image_analytics.pdf</td>\n",
       "      <td>24.29</td>\n",
       "      <td>70.0</td>\n",
       "      <td>19.35</td>\n",
       "      <td>34.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Resume  BERT Score  LLaMA Score  \\\n",
       "0             Dhruvraj_resume_MSDS.pdf       38.83         80.0   \n",
       "1            Dhruvraj_resume_May18.pdf       28.14         95.0   \n",
       "2         Dhruvraj_resume_USHunger.pdf       25.46         80.0   \n",
       "3    Dhruvraj_Resume_intern_rocket.pdf       28.24         70.0   \n",
       "4         Shefali_Bisht_Resume_BIE.pdf       35.62         50.0   \n",
       "5            Dhruvraj_resume_Mar11.pdf       29.52         60.0   \n",
       "6  Dhruvraj_resume_image_analytics.pdf       24.29         70.0   \n",
       "\n",
       "   Keyword Overlap  Combined Score  \n",
       "0            28.39           47.56  \n",
       "1            23.23           44.12  \n",
       "2            23.23           38.76  \n",
       "3            21.94           37.74  \n",
       "4            25.81           37.74  \n",
       "5            23.87           36.29  \n",
       "6            19.35           34.98  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score with BERT, LLAMA3.2 and keyword overlap with clean JD text for scoring:\n",
    "\n",
    "import os, re, PyPDF2, pandas as pd, nltk, torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "llm = ChatOllama(model='llama3.2')\n",
    "\n",
    "SECTION_MAP = {\n",
    "    \"skills\": [\"skills\", \"technical skills\", \"skill set\"],\n",
    "    \"experience\": [\"experience\", \"professional experience\", \"technical experience\", \"work experience\"],\n",
    "    \"projects\": [\"projects\", \"academic projects\", \"personal projects\"]\n",
    "}\n",
    "BASE_WEIGHTS = {\n",
    "    \"skills\": 0.10,\n",
    "    \"experience\": 0.50,\n",
    "    \"projects\": 0.30,\n",
    "    \"other\": 0.10\n",
    "}\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "JD_HEADERS = [\n",
    "    \"required qualifications\", \"preferred qualifications\", \"skills needed\", \"you will\",\n",
    "    \"job responsibilities\", \"minimum requirements\", \"what you'll work on\", \"what you bring\",\n",
    "    \"bonus point for\", \"key responsibilities\", \"requirements\", \"what you‚Äôll do\",\n",
    "    \"nice to have\", \"about you\", \"the following\"\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ helper functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def extract_text_from_pdf(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "def clean(line):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', '', line).strip().lower()\n",
    "\n",
    "def extract_sections(text: str) -> dict:\n",
    "    sections, current = {k: \"\" for k in BASE_WEIGHTS}, \"other\"\n",
    "    for raw in text.splitlines():\n",
    "        line = clean(raw)\n",
    "        if not line: continue\n",
    "        for key, aliases in SECTION_MAP.items():\n",
    "            if any(alias in line for alias in aliases):\n",
    "                current = key; break\n",
    "        sections[current] += raw + \" \"\n",
    "    return sections\n",
    "\n",
    "def keyword_overlap(jd, doc):\n",
    "    jd_kw  = {w for w in re.findall(r'\\w+', jd.lower())  if w not in STOPWORDS}\n",
    "    doc_kw = {w for w in re.findall(r'\\w+', doc.lower()) if w not in STOPWORDS}\n",
    "    return len(jd_kw & doc_kw) / max(len(jd_kw), 1)\n",
    "\n",
    "def llama_similarity(jd, exp_proj):\n",
    "    prompt = (\n",
    "        \"You're a hiring assistant. Based only on the experience & projects below, \"\n",
    "        \"rate the candidate's relevance to the job description on a scale 0-1. \"\n",
    "        \"Respond with **only** the number.\\n\\n\"\n",
    "        f\"Job Description:\\n{jd}\\n\\nExperience & Projects:\\n{exp_proj}\"\n",
    "    )\n",
    "    reply = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    match = re.search(r\"(?:0?\\.\\d+|1(?:\\.0+)?)\", reply)\n",
    "    return float(match.group()) if match else 0.0\n",
    "\n",
    "def dynamic_weights(sec_dict):\n",
    "    w = BASE_WEIGHTS.copy()\n",
    "    has_exp = sec_dict[\"experience\"].strip() != \"\"\n",
    "    has_prj = sec_dict[\"projects\"].strip() != \"\"\n",
    "    if not has_exp and has_prj:\n",
    "        w[\"projects\"] += w[\"experience\"]; w[\"experience\"] = 0.0\n",
    "    elif has_exp and not has_prj:\n",
    "        w[\"experience\"] += w[\"projects\"]; w[\"projects\"] = 0.0\n",
    "    return w\n",
    "\n",
    "def filter_jd_sections(text):\n",
    "    lines = text.splitlines()\n",
    "    collecting = False\n",
    "    collected = []\n",
    "    for line in lines:\n",
    "        l = line.strip().lower()\n",
    "        if any(header in l for header in JD_HEADERS):\n",
    "            collecting = True\n",
    "        if collecting:\n",
    "            collected.append(line.strip())\n",
    "    return \"\\n\".join(collected)\n",
    "\n",
    "def score_resume(fname, full_txt, secs, jd_txt):\n",
    "    w = dynamic_weights(secs)\n",
    "    jd_emb = bert_model.encode(jd_txt, convert_to_tensor=True)\n",
    "    bert_total = 0.0\n",
    "    for sec, txt in secs.items():\n",
    "        if not txt.strip() or w[sec] == 0: continue\n",
    "        emb = bert_model.encode(txt, convert_to_tensor=True)\n",
    "        sim = util.pytorch_cos_sim(emb, jd_emb).item()\n",
    "        bert_total += w[sec] * sim\n",
    "\n",
    "    kw_overlap = keyword_overlap(jd_txt, full_txt)\n",
    "    exp_proj = secs[\"experience\"] + \"\\n\" + secs[\"projects\"]\n",
    "    llama_sc = llama_similarity(jd_txt, exp_proj)\n",
    "    final = 0.60 * bert_total + 0.25 * llama_sc + 0.15 * kw_overlap\n",
    "\n",
    "    return dict(\n",
    "        Resume=fname,\n",
    "        **{\n",
    "            \"BERT Score\": round(bert_total * 100, 2),\n",
    "            \"LLaMA Score\": round(llama_sc * 100, 2),\n",
    "            \"Keyword Overlap\": round(kw_overlap * 100, 2),\n",
    "            \"Combined Score\": round(final * 100, 2)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ run pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "RES_DIR, JD_FILE = \"resumes/\", \"JDs/AI_intern_Armada.txt\"#\"JDs/Data_engineering_intern_LiveRamp.txt\"\n",
    "jd_raw = open(JD_FILE, encoding='utf-8').read()\n",
    "jd_text = filter_jd_sections(jd_raw)\n",
    "# print(\"\\nüìù Filtered Job Description used for scoring:\\n\")\n",
    "# print(jd_text)\n",
    "# print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "results = []\n",
    "for fn in os.listdir(RES_DIR):\n",
    "    if not fn.endswith(\".pdf\"): continue\n",
    "    print(\"‚Üí\", fn)\n",
    "    txt = extract_text_from_pdf(os.path.join(RES_DIR, fn))\n",
    "    secs = extract_sections(txt)\n",
    "    results.append(score_resume(fn, txt, secs, jd_text))\n",
    "\n",
    "df = pd.DataFrame(results).sort_values(\"Combined Score\", ascending=False).reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
